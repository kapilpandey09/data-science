{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da5f694b-fa4e-46b3-a3d4-c2045e1b4380",
   "metadata": {},
   "source": [
    "## Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ead06d-3ef8-4971-bb6e-9af59c900faa",
   "metadata": {},
   "source": [
    "\n",
    "Ridge Regression, also known as Tikhonov regularization or L2 regularization, is a linear regression technique that introduces a regularization term to the ordinary least squares (OLS) objective function. The regularization term is proportional to the square of the magnitude of the coefficients, and it is added to the sum of squared residuals.\n",
    "\n",
    "Here, the first part of the equation is the ordinary least squares objective, and the second part is the regularization term. The parameter Î± controls the strength of the regularization, with higher values leading to more regularization.\n",
    "\n",
    "The key difference between Ridge Regression and ordinary least squares regression lies in the regularization term. In OLS, there is no regularization term, and the goal is to minimize the sum of squared residuals only. Ridge Regression, on the other hand, adds a penalty for large coefficient values, helping to prevent overfitting by discouraging overly complex models.\n",
    "\n",
    "The regularization term in Ridge Regression is particularly useful when dealing with multicollinearity, a situation where predictor variables are highly correlated. In such cases, OLS may produce unstable and unreliable estimates of the coefficients, whereas Ridge Regression provides more stable estimates by constraining the coefficients.\n",
    "\n",
    "In summary, Ridge Regression extends ordinary least squares regression by introducing a regularization term that penalizes large coefficient values, providing a balance between fitting the data well and preventing overfitting, especially in the presence of multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57d7360-6879-4df0-b322-fa6b16a1c8bd",
   "metadata": {},
   "source": [
    "## Q2. What are the assumptions of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4a48bf-46ba-4ec6-b91c-69dfe9bcb738",
   "metadata": {},
   "source": [
    "The assumptions of Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5932500b-1c05-4a64-88b6-a4b3d6c7a10d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debba53e-c6b4-4edb-bb1f-009ad29054a9",
   "metadata": {},
   "source": [
    "acording to best fit line value "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03103dce-c66b-4a2c-8122-5f51cc3e853a",
   "metadata": {},
   "source": [
    "## Q4. Can Ridge Regression be used for feature selection? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f48aeb-6f02-4588-a2d7-4d0529f33bd4",
   "metadata": {},
   "source": [
    "No This is a used overfitibg value and does not use feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1c9851-c705-48b5-af5a-5770cd32d4d5",
   "metadata": {},
   "source": [
    "## Q5. How does the Ridge Regression model perform in the presence of multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11205b54-884b-4426-895b-79266f21ee6c",
   "metadata": {},
   "source": [
    "Any no idea what is perfomance but good perform this data multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d296eb8f-df63-4eb9-9985-68aa827ed013",
   "metadata": {},
   "source": [
    "## Q6. Can Ridge Regression handle both categorical and continuous independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccc2198-2012-4904-8df5-96d022290992",
   "metadata": {},
   "source": [
    "No"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da8e11b-cd06-4e14-8b42-728eb85beb95",
   "metadata": {},
   "source": [
    "## Q7. How do you interpret the coefficients of Ridge Regression?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96dfd66-7004-4f19-9d97-80069619eb80",
   "metadata": {},
   "source": [
    "In Ridge Regression, the coefficients are interpreted in a similar way to linear regression. However, Ridge Regression introduces a regularization term to the linear regression cost function, which adds a penalty for large coefficients. This regularization term helps to prevent overfitting by shrinking the coefficients towards zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44452f1-bb3f-4d68-a3f4-5efe71c1e382",
   "metadata": {},
   "source": [
    "## Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26ba315-a82d-48e7-bf43-705424163b83",
   "metadata": {},
   "source": [
    "Yes, Ridge Regression can be used for time-series data analysis. Time-series data involves observations taken over time, and regression models like Ridge Regression can be applied to analyze and make predictions based on such data.\n",
    "\n",
    "- Feature Selection and Engineering:\n",
    "- Data Splitting:\n",
    "- Normalization:\n",
    "- Ridge Regression Model:\n",
    "- Hyperparameter Tuning:\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdebf07e-622a-411e-90b6-b2b7b0b1c3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
