{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ba08437-11ec-40a0-bb0c-9ceaa7f856df",
   "metadata": {},
   "source": [
    "# Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73275cce-8936-46ad-adcc-51ebeb5be1c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Simple Linear Regression (SLR):\n",
    "\n",
    "- Involves one independent variable (X) and one dependent variable (Y).\n",
    "- Models a straight-line relationship between X and Y.\n",
    "- Equation: Y = β0 + β1X + ε (where β0 is the intercept, β1 is the slope, and ε is the error term).\n",
    "### Example:\n",
    "- Predicting house prices based on square footage.\n",
    "- Investigating the relationship between student's study hours and exam scores.\n",
    "- Examining the association between daily exercise time and weight loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f004768-e127-418e-ad60-a69666b89708",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Multiple Linear Regression (MLR):\n",
    "\n",
    "- Involves two or more independent variables (X1, X2, ..., Xp) and one dependent variable (Y).\n",
    "- Models a linear relationship between Y and the combination of X variables.\n",
    "- Equation: Y = β0 + β1X1 + β2X2 + ... + βpXp + ε\n",
    "### Example:\n",
    "\n",
    "- Predicting employee salaries based on years of experience, education level, and job title.\n",
    "- Modeling disease risk based on age, blood pressure, cholesterol levels, and smoking habits.\n",
    "- Analyzing crop yield based on factors like rainfall, fertilizer use, temperature, and soil quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc31ced-1308-41b8-9a57-a28243662f33",
   "metadata": {},
   "source": [
    "# Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5ab03e-3ccd-41fe-b61d-12745d1763dc",
   "metadata": {},
   "source": [
    "### Linearity:\n",
    "\n",
    "- Assumption: The relationship between the independent variables (X) and the dependent variable (Y) is linear.\n",
    "\n",
    "#### Checking:\n",
    "\n",
    "- Scatter plots: Visually examine the relationship between X and Y for linearity.\n",
    "- Residual plots: Plot residuals (differences between actual and predicted values) against predicted values. Look for random scatter without patterns.\n",
    "### 2. Independence:\n",
    "\n",
    "- Assumption: Observations are independent of each other.\n",
    "#### Checking:\n",
    "- Durbin-Watson test: Detects autocorrelation (correlation between residuals).\n",
    "- Consider study design and data collection methods to assess potential independence issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e842e0-9ea1-4785-8d3c-a650d8b28ab8",
   "metadata": {},
   "source": [
    "# Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72830123-2f9f-44b2-934f-ca7849b1951c",
   "metadata": {},
   "source": [
    "#### Slope:\n",
    "\n",
    "- It describes the change in the independent variable (x) and the dependent variable.\n",
    "- Tells you how much y is expected to change for every 1-unit increase in x.\n",
    "- Positive slope: Indicates a positive relationship (as x increases, y increases).\n",
    "- Negative slope: Indicates a negative relationship (as x increases, y decreases).\n",
    "- Zero slope: Indicates no relationship between x and y.\n",
    "\n",
    "#### Intercept\n",
    "\n",
    "- When x axis is zero, there will be some value  of y axis which is called intercept.\n",
    "- It is a point where the regression line crosses the Y-axis\n",
    "\n",
    "#### Real World Example:\n",
    "\n",
    "- Scenario: Predicting house prices based on square footage.\n",
    "\n",
    "- Regression equation: ŷ = 50,000 + 100x\n",
    "\n",
    "- Slope (100): For every additional square foot of living space, the predicted house price increases by $100 dollar.\n",
    "- Intercept (50,000): The predicted price of a house with 0 square feet (which doesn't make practical sense) is $50,000 dollar. However, it serves as a baseline for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3af4bb1-9442-4377-a2eb-5b2d60117707",
   "metadata": {},
   "source": [
    "# Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfd8e4d-8b3d-4f97-ae6d-8db5bb8fe76c",
   "metadata": {},
   "source": [
    "Imagine you're lost in a hilly landscape and searching for the valley floor (the lowest point). Gradient descent is like your guide, helping you navigate down the steepest slopes towards the valley bottom. This is essentially what it does in machine learning: finding the optimal settings for a model by minimizing a \"cost function\".\n",
    "\n",
    "In summary, gradient descent is a powerful optimization algorithm that guides machine learning models towards optimal parameter settings by minimizing the cost function. It's like your guide in a hilly landscape, helping you find the best path to the lowest point (optimal model performance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11195364-37f9-4397-9bb3-dd41a4b1da57",
   "metadata": {},
   "source": [
    "# Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5627d044-75a0-4e44-bfdb-8d554a1ff218",
   "metadata": {},
   "source": [
    "#### Multiple linear regression: \n",
    "- This is when there are multiple feature datasets in which multiple linear regression are used.\n",
    "#### Simple linear regression: \n",
    "- this used to two feature datset model which are used to single linear regression\n",
    "\n",
    "#### Difference :\n",
    "\n",
    "- both are difference because both are used different regresstion model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e4a39a-1d99-4037-a132-39b6b1b95b5c",
   "metadata": {},
   "source": [
    "# Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05b60e9-789c-4d8e-85a9-725002875ed6",
   "metadata": {},
   "source": [
    "\n",
    "Multicollinearity in Multiple Linear Regression\n",
    "\n",
    "Imagine a tangled web of variables: that's multicollinearity. It occurs when two or more independent variables (predictors) in a multiple linear regression model are highly correlated with each other. This can make it difficult to isolate the individual effects of each variable on the dependent variable (the outcome you're trying to predict).\n",
    "\n",
    "Problems Caused by Multicollinearity:\n",
    "\n",
    "Unstable and unreliable coefficient estimates: The model can't confidently determine the unique contribution of each correlated variable, leading to coefficients that can change dramatically with small changes in the data.\n",
    "Wide confidence intervals: This makes it harder to determine the statistical significance of individual variables.\n",
    "Misleading interpretations: You might think a variable is unimportant when it's actually highly correlated with another important variable.\n",
    "Overfitting: The model might fit the training data too closely, but perform poorly on new data.\n",
    "Detecting Multicollinearity:\n",
    "\n",
    "1. Correlation Matrix: Examine the correlations between independent variables. High correlations (e.g., above 0.8 or 0.9) suggest multicollinearity.\n",
    "\n",
    "2. Variance Inflation Factor (VIF): VIF measures how much the variance of a coefficient is inflated due to multicollinearity. VIF values above 5 or 10 are often considered problematic.\n",
    "\n",
    "Addressing Multicollinearity:\n",
    "\n",
    "1. Remove Correlated Variables: Consider removing one or more of the highly correlated variables from the model. Choose the variables that are less important for prediction or interpretation.\n",
    "\n",
    "2. Combine Variables: If correlated variables represent similar concepts, consider combining them into a single composite variable.\n",
    "\n",
    "3. Regularization: Techniques like ridge regression or LASSO can shrink coefficients and reduce the impact of multicollinearity.\n",
    "\n",
    "4. Increase Sample Size: Larger samples can sometimes reduce the effects of multicollinearity.\n",
    "\n",
    "5. Principal Component Analysis (PCA): Transform correlated variables into uncorrelated principal components.\n",
    "\n",
    "Remember: It's important to address multicollinearity to ensure reliable and interpretable regression results. By understanding and addressing this issue, you can build more robust and accurate models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1cc0cf-8784-40de-9b39-6337bdfa6afa",
   "metadata": {},
   "source": [
    "# Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74360fdc-b497-4d5a-beb5-667f3047b2dc",
   "metadata": {},
   "source": [
    "#### Polynomial regression is a model this model based on the curve nature . \n",
    "- Polynomial regression different to linear regression this used to curve nature and linear regression used to simple line which pass a dataset and achieve best fit line "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccd6267-08bb-456f-a4c4-3dbd3e3e772a",
   "metadata": {},
   "source": [
    "# Q8. What are the advantages and disadvantages of polynomial regression compared to linearb regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449bc9ed-dc10-44e8-b68e-7ab6b91eb12f",
   "metadata": {},
   "source": [
    "# Advantage:\n",
    "- when a dataset a curvenature then use a polynomial regression\n",
    "# disadvantage:\n",
    "- when a dataset nature simple line so then use simple line regreesion\n",
    "\n",
    "#### when a dataset nature a curve so then use poynomial regression \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d863a64-39e6-43b5-96ae-b91b54216462",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
